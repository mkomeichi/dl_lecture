{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def diff_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp)\n",
    "def diff_softmax(x):\n",
    "    return softmax(x)*(np.ones(x.shape)-softmax(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "def diff_tanh(x):\n",
    "    return 1-tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,in_dim,out_dim,function,diff_function):\n",
    "        #Xavier\n",
    "        self.W = np.random.uniform(\n",
    "                                    low=-np.sqrt(6./(in_dim+out_dim)), \n",
    "                                    high=np.sqrt(6./(in_dim+out_dim)), \n",
    "                                    size=(in_dim, out_dim))\n",
    "        self.b = np.zeros(out_dim)\n",
    "        self.function = function\n",
    "        \n",
    "        self.diff_function = diff_function\n",
    "        self.u     = None\n",
    "        self.delta = None\n",
    "\n",
    "    #foward propagation\n",
    "    def fprop(self,x):\n",
    "        u = np.dot(x,self.W)+self.b\n",
    "        z = self.function(u)\n",
    "        self.u = u\n",
    "        return z\n",
    "\n",
    "    #back propagation\n",
    "    def bprop(self,delta,W):\n",
    "        delta = self.diff_function(self.u)*np.dot(delta,W.T)\n",
    "        self.delta = delta\n",
    "        return delta\n",
    "\n",
    "#ネットワーク全体の順伝播\n",
    "def fprops(layers, x):\n",
    "    z = x\n",
    "    for layer in layers:\n",
    "        z = layer.fprop(z)\n",
    "    return z\n",
    "\n",
    "#ネットワーク全体の誤差逆伝播\n",
    "def bprops(layers, delta):\n",
    "    for i,layer in enumerate(layers[::-1]):\n",
    "        if i==0:\n",
    "            layer.delta = delta\n",
    "        else:\n",
    "            delta = layer.bprop(delta,_W)\n",
    "        _W = layer.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train関数とtest関数\n",
    "def train(X,d,eps=0.01):\n",
    "    #forward propagation\n",
    "    y = fprops(layers,X)\n",
    "        \n",
    "    #cost function & delta\n",
    "    cost = np.sum(-d*np.log(y)-(1-d)*np.log(1-y))\n",
    "    delta = y-d\n",
    "    \n",
    "    #back propagation\n",
    "    bprops(layers,delta)\n",
    "\n",
    "    #update parameters\n",
    "    z = X\n",
    "\n",
    "    for layer in layers:\n",
    "        layer.delta = np.atleast_2d(layer.delta)\n",
    "        z = np.atleast_2d(z)\n",
    "        dW = np.dot(z.T, layer.delta)\n",
    "        db = np.dot(np.ones(len(z)),layer.delta)\n",
    "\n",
    "        layer.W = layer.W - eps*dW\n",
    "        layer.b = layer.b - eps*db\n",
    "\n",
    "        z = layer.fprop(z)\n",
    "        \n",
    "    #train cost\n",
    "    y = fprops(layers,X)\n",
    "    cost = np.sum(-d*np.log(y)-(1-d)*np.log(1-y))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def test(X):\n",
    "    #test cost\n",
    "    y = fprops(layers,X)\n",
    "    #cost = np.sum(-d*np.log(y)-(1-d)*np.log(1-y))\n",
    "    #return cost,y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.58632390323e-05\n",
      "[[1326    0   49    9    1   31    9    5    5    0]\n",
      " [   0 1518   16    0    6    6    3    8    3    1]\n",
      " [   2    3 1320    6    6    4    3    8   13    0]\n",
      " [   1    2   41 1290    1   40    2   16   23    6]\n",
      " [   2    0   24    1 1322    3   10   11    5   16]\n",
      " [   3    0    5   10    7 1172    7    3    9    5]\n",
      " [   3    0   32    3   16   24 1310    0    2    0]\n",
      " [   2    6   16    4   10    4    0 1433    1   16]\n",
      " [   5   31   69   31   22   67    5    8 1127    9]\n",
      " [   8    8   11   16   69   15    0   61   18 1140]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.92      0.95      1435\n",
      "        1.0       0.97      0.97      0.97      1561\n",
      "        2.0       0.83      0.97      0.90      1365\n",
      "        3.0       0.94      0.91      0.92      1422\n",
      "        4.0       0.91      0.95      0.93      1394\n",
      "        5.0       0.86      0.96      0.91      1221\n",
      "        6.0       0.97      0.94      0.96      1390\n",
      "        7.0       0.92      0.96      0.94      1492\n",
      "        8.0       0.93      0.82      0.87      1374\n",
      "        9.0       0.96      0.85      0.90      1346\n",
      "\n",
      "avg / total       0.93      0.93      0.93     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 出力yはone-of-k表現\n",
    "# 最終層の活性化関数はsoftmax関数，誤差関数は多クラス交差エントロピー\n",
    "# 最終層のデルタは教科書参照\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X, y = shuffle(mnist.data, mnist.target)\n",
    "X = X / 255.0\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 教師信号の数字を1-of-K表記に変換\n",
    "labels_train = LabelBinarizer().fit_transform(train_y)\n",
    "#labels_test = LabelBinarizer().fit_transform(test_y)\n",
    "\n",
    "layers = [Layer(28*28,100,tanh,diff_tanh),\n",
    "          Layer(100,10,softmax,diff_softmax)]\n",
    "\n",
    "#パラメータの更新 学習\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    #print epoch\n",
    "    #online learning\n",
    "    #train_X, train_y = shuffle(train_X, labels_train)\n",
    "    for x,y in zip(train_X,labels_train):\n",
    "        cost = train(x,y)\n",
    "#cost,pred_y = test(test_X,test_y)\n",
    "    print cost\n",
    "\n",
    "# テストデータを用いて予測精度を計算\n",
    "predictions = []\n",
    "for i in range(test_X.shape[0]):\n",
    "    o = test(test_X[i])\n",
    "    predictions.append(np.argmax(o))\n",
    "print confusion_matrix(test_y, predictions)\n",
    "print classification_report(test_y, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
