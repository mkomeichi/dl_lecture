{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://aidiary.hatenablog.com/entry/20140201/1391218771\n",
    "#coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "mlp.py\n",
    "多層パーセプトロン\n",
    "forループの代わりに行列演算にした高速化版\n",
    "\n",
    "入力層 - 隠れ層 - 出力層の3層構造で固定（PRMLではこれを2層と呼んでいる）\n",
    "\n",
    "隠れ層の活性化関数にはtanh関数またはsigmoid logistic関数が使える\n",
    "出力層の活性化関数にはtanh関数、sigmoid logistic関数、恒等関数が使える\n",
    "\"\"\"\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# 本来はtanh'(x) = 1 - tanh(x) ** 2であるが\n",
    "# このスクリプトでは引数xがtanh(x)であることを仮定している\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - x ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 本来はsigmoid'(x) = sigmoid(x) * (1 - sigmoid(x))であるが\n",
    "# このスクリプトでは引数xがsigmoid(x)であることを仮定している\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp)\n",
    "\n",
    "#def softmax(x):\n",
    "#    return np.exp(x)/np.sum(np.exp(x),axis=1)[:,np.newaxis]\n",
    "\n",
    "def softmax_deriv(x):\n",
    "    return softmax(x)*(np.ones(x.shape)-softmax(x))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def identity_deriv(x):\n",
    "    return 1\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, numInput, numHidden, numOutput, act1=\"tanh\", act2=\"sigmoid\"):\n",
    "        \"\"\"多層パーセプトロンを初期化\n",
    "        numInput    入力層のユニット数（バイアスユニットは除く）\n",
    "        numHidden   隠れ層のユニット数（バイアスユニットは除く）\n",
    "        numOutput   出力層のユニット数\n",
    "        act1        隠れ層の活性化関数（tanh or sigmoid）\n",
    "        act2        出力層の活性化関数（tanh or sigmoid or identity）\n",
    "        \"\"\"\n",
    "        # 引数の指定に合わせて隠れ層の活性化関数とその微分関数を設定\n",
    "        if act1 == \"tanh\":\n",
    "            self.act1 = tanh\n",
    "            self.act1_deriv = tanh_deriv\n",
    "        elif act1 == \"sigmoid\":\n",
    "            self.act1 = sigmoid\n",
    "            self.act1_deriv = sigmoid_deriv\n",
    "        else:\n",
    "            print \"ERROR: act1 is tanh or sigmoid\"\n",
    "            sys.exit()\n",
    "\n",
    "        # 引数の指定に合わせて出力層の活性化関数とその微分関数を設定\n",
    "        if act2 == \"tanh\":\n",
    "            self.act2 = tanh\n",
    "            self.act2_deriv = tanh_deriv\n",
    "        elif act2 == \"sigmoid\":\n",
    "            self.act2 = sigmoid\n",
    "            self.act2_deriv = sigmoid_deriv\n",
    "        elif act2 == \"softmax\":\n",
    "            self.act2 = softmax\n",
    "            self.act2_deriv = softmax_deriv\n",
    "        elif act2 == \"identity\":\n",
    "            self.act2 = identity\n",
    "            self.act2_deriv = identity_deriv\n",
    "        else:\n",
    "            print \"ERROR: act2 is tanh or sigmoid or or softmax or identity\"\n",
    "            sys.exit()\n",
    "\n",
    "        # バイアスユニットがあるので入力層と隠れ層は+1\n",
    "        self.numInput = numInput + 1\n",
    "        self.numHidden =numHidden + 1\n",
    "        self.numOutput = numOutput\n",
    "\n",
    "        # 重みを (-1.0, 1.0)の一様乱数で初期化\n",
    "        self.weight1 = np.random.uniform(-1.0, 1.0, (self.numHidden, self.numInput))  # 入力層-隠れ層間\n",
    "        self.weight2 = np.random.uniform(-1.0, 1.0, (self.numOutput, self.numHidden)) # 隠れ層-出力層間\n",
    "\n",
    "    def fit(self, X, t, learning_rate=0.2, epochs=10000):\n",
    "        \"\"\"訓練データを用いてネットワークの重みを更新する\"\"\"\n",
    "        # 入力データの最初の列にバイアスユニットの入力1を追加\n",
    "        X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "        t = np.array(t)\n",
    "\n",
    "        # 逐次学習\n",
    "        # 訓練データからランダムサンプリングして重みを更新をepochs回繰り返す\n",
    "        for k in range(epochs):\n",
    "            #print k\n",
    "\n",
    "            # 訓練データからランダムに選択する\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            x = X[i]\n",
    "\n",
    "            # 入力を順伝播させて中間層の出力を計算\n",
    "            z = self.act1(np.dot(self.weight1, x))\n",
    "\n",
    "            # 中間層の出力を順伝播させて出力層の出力を計算\n",
    "            y = self.act2(np.dot(self.weight2, z))\n",
    "\n",
    "            # 出力層の誤差を計算\n",
    "            # WARNING\n",
    "            # PRMLによると出力層の活性化関数にどれを用いても\n",
    "            # (y - t[i]) でよいと書いてあるが\n",
    "            # 下のように出力層の活性化関数の微分もかけた方が精度がずっとよくなる\n",
    "            # delta2 = self.act2_deriv(y) * (y - t[i])\n",
    "            #誤差関数に交差エントロピー誤差関数を利用\n",
    "            delta2 = y - t[i]\n",
    "            \n",
    "            # 出力層の誤差を逆伝播させて隠れ層の誤差を計算\n",
    "            delta1 = self.act1_deriv(z) * np.dot(self.weight2.T, delta2)\n",
    "\n",
    "            # 隠れ層の誤差を用いて隠れ層の重みを更新\n",
    "            # 行列演算になるので2次元ベクトルに変換する必要がある\n",
    "            x = np.atleast_2d(x)\n",
    "            delta1 = np.atleast_2d(delta1)\n",
    "            self.weight1 -= learning_rate * np.dot(delta1.T, x)\n",
    "\n",
    "            # 出力層の誤差を用いて出力層の重みを更新\n",
    "            z = np.atleast_2d(z)\n",
    "            delta2 = np.atleast_2d(delta2)\n",
    "            self.weight2 -= learning_rate * np.dot(delta2.T, z)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"テストデータの出力を予測\"\"\"\n",
    "        x = np.array(x)\n",
    "        # バイアスの1を追加\n",
    "        x = np.insert(x, 0, 1)\n",
    "        # 順伝播によりネットワークの出力を計算\n",
    "        z = self.act1(np.dot(self.weight1, x))\n",
    "        y = self.act2(np.dot(self.weight2, z))\n",
    "        return y\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    \"\"\"XORの学習テスト\"\"\"\n",
    "#    mlp = MultiLayerPerceptron(2, 2, 1, \"tanh\", \"sigmoid\")\n",
    "#    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "#    y = np.array([0, 1, 1, 0])\n",
    "#    mlp.fit(X, y)\n",
    "#    for i in [[0, 0], [0, 1], [1, 0], [1, 1]]:\n",
    "#        print i, mlp.predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[674   0   2   0   1  13  10   0   0   2]\n",
      " [  0 744   4   1   3   1   2   4   1   1]\n",
      " [  4   7 674   4   7   5   6   9  13   2]\n",
      " [  3   3  27 666   1  28   1   8   9   9]\n",
      " [  1   2   4   1 658   4   3   2   4  19]\n",
      " [  2   4   4  14   4 603   8   2   6   4]\n",
      " [  7   1   1   0  12   7 655   1   1   0]\n",
      " [  2   3  10   0   3   4   0 647   1  30]\n",
      " [  6   7  15  17   3  23  12   7 577  15]\n",
      " [  5   1   1   3  27   1   0  27   5 565]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.96      0.96       702\n",
      "        1.0       0.96      0.98      0.97       761\n",
      "        2.0       0.91      0.92      0.92       731\n",
      "        3.0       0.94      0.88      0.91       755\n",
      "        4.0       0.92      0.94      0.93       698\n",
      "        5.0       0.88      0.93      0.90       651\n",
      "        6.0       0.94      0.96      0.95       685\n",
      "        7.0       0.92      0.92      0.92       700\n",
      "        8.0       0.94      0.85      0.89       682\n",
      "        9.0       0.87      0.89      0.88       635\n",
      "\n",
      "avg / total       0.92      0.92      0.92      7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "import numpy as np\n",
    "#from mlp import MultiLayerPerceptron\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\"\"\"\n",
    "MNISTの手書き数字データの認識\n",
    "scikit-learnのインストールが必要\n",
    "http://scikit-learn.org/\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # MNISTの数字データ\n",
    "    # 70000サンプル, 28x28ピクセル\n",
    "    # カレントディレクトリ（.）にmnistデータがない場合は\n",
    "    # Webから自動的にダウンロードされる（時間がかかる）\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "    # 訓練データを作成\n",
    "    X = mnist.data\n",
    "    y = mnist.target\n",
    "\n",
    "    # ピクセルの値を0.0-1.0に正規化\n",
    "    X = X.astype(np.float64)\n",
    "    X /= X.max()\n",
    "\n",
    "    # 多層パーセプトロンを構築\n",
    "    mlp = MultiLayerPerceptron(28*28, 100, 10, \"tanh\", \"softmax\")\n",
    "\n",
    "    # 訓練データとテストデータに分解\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "    # 教師信号の数字を1-of-K表記に変換\n",
    "    labels_train = LabelBinarizer().fit_transform(y_train)\n",
    "    labels_test = LabelBinarizer().fit_transform(y_test)\n",
    "\n",
    "    # 訓練データを用いてニューラルネットの重みを学習\n",
    "    mlp.fit(X_train, labels_train, learning_rate=0.01, epochs=100000)\n",
    "\n",
    "    # テストデータを用いて予測精度を計算\n",
    "    predictions = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        o = mlp.predict(X_test[i])\n",
    "        predictions.append(np.argmax(o))\n",
    "    print confusion_matrix(y_test, predictions)\n",
    "    print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
